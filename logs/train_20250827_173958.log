2025-08-27 17:39:58 - INFO - Training with args: checkpoint_dir: checkpoints_v2
2025-08-27 17:39:58 - INFO - Training with args: resume_from: None
2025-08-27 17:39:58 - INFO - Training with args: model_name: answerdotai/ModernBERT-base
2025-08-27 17:39:58 - INFO - Training with args: tokenizer_name: ricardoz/BERTugues-base-portuguese-cased
2025-08-27 17:39:58 - INFO - Training with args: dataset_name: Itau-Unibanco/aroeira
2025-08-27 17:39:58 - INFO - Training with args: optimizer: adamw
2025-08-27 17:39:58 - INFO - Training with args: train_part: pt1
2025-08-27 17:39:58 - INFO - Training with args: rope_theta: 10000.0
2025-08-27 17:39:58 - INFO - Training with args: warmup_pct: 0.05
2025-08-27 17:39:58 - INFO - Training with args: decay_type: cosine
2025-08-27 17:39:58 - INFO - Training with args: decay_pct: 0.9
2025-08-27 17:39:58 - INFO - Training with args: min_lr: 1e-08
2025-08-27 17:39:58 - INFO - Training with args: epochs: 2
2025-08-27 17:39:58 - INFO - Training with args: total_samples: 34841241
2025-08-27 17:39:58 - INFO - Training with args: max_length: 1024
2025-08-27 17:39:58 - INFO - Training with args: batch_size: 16
2025-08-27 17:39:58 - INFO - Training with args: mlm_probability: 0.15
2025-08-27 17:39:58 - INFO - Training with args: mask_replace_prob: 1.0
2025-08-27 17:39:58 - INFO - Training with args: random_replace_prob: 0.0
2025-08-27 17:39:58 - INFO - Training with args: grad_accum: 72
2025-08-27 17:39:58 - INFO - Training with args: lr: 0.0008
2025-08-27 17:39:58 - INFO - Training with args: save_every: 100000
2025-08-27 17:39:58 - INFO - Training with args: log_every: 200
2025-08-27 17:39:58 - INFO - Training with args: log_dir: logs
2025-08-27 17:39:58 - INFO - Loading Config from checkpoints_v2/ckpt-299999 with rope_theta=10000.0
2025-08-27 17:39:58 - INFO - Resuming from checkpoint: checkpoints_v2/ckpt-299999
2025-08-27 17:40:00 - INFO - Initializing Data Loader
2025-08-27 17:40:02 - INFO - Getting data from 0th sentence to 30000000th
2025-08-27 17:40:02 - INFO - Starting training from step 299999
2025-08-27 17:40:02 - INFO - Initializing model with Accelerator
2025-08-27 17:40:03 - INFO - Initializing training
2025-08-27 17:40:03 - INFO - Dataset samples for training: 30000000
2025-08-27 17:40:03 - INFO - No. GPUs: 4
2025-08-27 17:40:03 - INFO - Batch Size per GPU: 16
2025-08-27 17:40:03 - INFO - Steps in dataset: 468750
2025-08-27 17:40:03 - INFO - Epochs: 2
2025-08-27 17:40:03 - INFO - Total Steps: 937500
2025-08-27 17:40:03 - INFO - Resuming from global step 299999
