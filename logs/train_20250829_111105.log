2025-08-29 11:11:05 - INFO - Training with args: checkpoint_dir: checkpoints_v2
2025-08-29 11:11:05 - INFO - Training with args: resume_from: None
2025-08-29 11:11:05 - INFO - Training with args: model_name: answerdotai/ModernBERT-base
2025-08-29 11:11:05 - INFO - Training with args: tokenizer_name: ricardoz/BERTugues-base-portuguese-cased
2025-08-29 11:11:05 - INFO - Training with args: dataset_name: Itau-Unibanco/aroeira
2025-08-29 11:11:05 - INFO - Training with args: optimizer: adamw
2025-08-29 11:11:05 - INFO - Training with args: train_part: pt1
2025-08-29 11:11:05 - INFO - Training with args: rope_theta: 10000.0
2025-08-29 11:11:05 - INFO - Training with args: warmup_pct: 0.05
2025-08-29 11:11:05 - INFO - Training with args: decay_type: cosine
2025-08-29 11:11:05 - INFO - Training with args: decay_pct: 0.9
2025-08-29 11:11:05 - INFO - Training with args: min_lr: 1e-08
2025-08-29 11:11:05 - INFO - Training with args: epochs: 2
2025-08-29 11:11:05 - INFO - Training with args: total_samples: 34841241
2025-08-29 11:11:05 - INFO - Training with args: max_length: 1024
2025-08-29 11:11:05 - INFO - Training with args: batch_size: 16
2025-08-29 11:11:05 - INFO - Training with args: mlm_probability: 0.15
2025-08-29 11:11:05 - INFO - Training with args: mask_replace_prob: 1.0
2025-08-29 11:11:05 - INFO - Training with args: random_replace_prob: 0.0
2025-08-29 11:11:05 - INFO - Training with args: grad_accum: 72
2025-08-29 11:11:05 - INFO - Training with args: lr: 0.0008
2025-08-29 11:11:05 - INFO - Training with args: save_every: 100000
2025-08-29 11:11:05 - INFO - Training with args: log_every: 200
2025-08-29 11:11:05 - INFO - Training with args: log_dir: logs
2025-08-29 11:11:05 - INFO - Loading Config from checkpoints_v2/ckpt-468750 with rope_theta=10000.0
2025-08-29 11:11:05 - INFO - Resuming from checkpoint: checkpoints_v2/ckpt-468750
2025-08-29 11:11:07 - INFO - Initializing Data Loader
2025-08-29 11:11:10 - INFO - Getting data from 0th sentence to 30000000th
2025-08-29 11:11:10 - INFO - Starting training from step 468750
2025-08-29 11:11:10 - INFO - Initializing model with Accelerator
2025-08-29 11:11:10 - INFO - Initializing training
2025-08-29 11:11:10 - INFO - Dataset samples for training: 30000000
2025-08-29 11:11:10 - INFO - No. GPUs: 4
2025-08-29 11:11:10 - INFO - Batch Size per GPU: 16
2025-08-29 11:11:10 - INFO - Steps in dataset: 468750
2025-08-29 11:11:10 - INFO - Epochs: 2
2025-08-29 11:11:10 - INFO - Total Steps: 937500
2025-08-29 11:11:10 - INFO - Resuming from global step 468750
2025-08-29 11:11:36 - INFO - [BEST] step=468792 ema_loss=0.865095 (salvo em 'best')
2025-08-29 11:11:41 - INFO - Epoch 2 - Step 468800/937500 (50.01 %) - Loss(step) 0.89372 - EMA 0.87882 - LR 4.40e-04 - Time: 0h 0m 30s
