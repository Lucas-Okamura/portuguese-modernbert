2025-08-29 10:56:23 - INFO - Training with args: checkpoint_dir: checkpoints_v2
2025-08-29 10:56:23 - INFO - Training with args: resume_from: None
2025-08-29 10:56:23 - INFO - Training with args: model_name: answerdotai/ModernBERT-base
2025-08-29 10:56:23 - INFO - Training with args: tokenizer_name: ricardoz/BERTugues-base-portuguese-cased
2025-08-29 10:56:23 - INFO - Training with args: dataset_name: Itau-Unibanco/aroeira
2025-08-29 10:56:23 - INFO - Training with args: optimizer: adamw
2025-08-29 10:56:23 - INFO - Training with args: train_part: pt1
2025-08-29 10:56:23 - INFO - Training with args: rope_theta: 10000.0
2025-08-29 10:56:23 - INFO - Training with args: warmup_pct: 0.05
2025-08-29 10:56:23 - INFO - Training with args: decay_type: cosine
2025-08-29 10:56:23 - INFO - Training with args: decay_pct: 0.9
2025-08-29 10:56:23 - INFO - Training with args: min_lr: 1e-08
2025-08-29 10:56:23 - INFO - Training with args: epochs: 2
2025-08-29 10:56:23 - INFO - Training with args: total_samples: 34841241
2025-08-29 10:56:23 - INFO - Training with args: max_length: 1024
2025-08-29 10:56:23 - INFO - Training with args: batch_size: 16
2025-08-29 10:56:23 - INFO - Training with args: mlm_probability: 0.15
2025-08-29 10:56:23 - INFO - Training with args: mask_replace_prob: 1.0
2025-08-29 10:56:23 - INFO - Training with args: random_replace_prob: 0.0
2025-08-29 10:56:23 - INFO - Training with args: grad_accum: 72
2025-08-29 10:56:23 - INFO - Training with args: lr: 0.0008
2025-08-29 10:56:23 - INFO - Training with args: save_every: 100000
2025-08-29 10:56:23 - INFO - Training with args: log_every: 200
2025-08-29 10:56:23 - INFO - Training with args: log_dir: logs
2025-08-29 10:56:23 - INFO - Loading Config from checkpoints_v2/ckpt-468750 with rope_theta=10000.0
2025-08-29 10:56:23 - INFO - Resuming from checkpoint: checkpoints_v2/ckpt-468750
